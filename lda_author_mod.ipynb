{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/baddie/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/Users/baddie/opt/anaconda3/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "################################################################ TO CHANGE ################################################################\n",
    "# Stolen from Sneha\n",
    "# FILE PATHS AND FIELDS\n",
    "path_json = \"comp767_papers_sample.jsonl\"  #3154 papers\n",
    "fields = [\"title\", \"abstract\", \"authors\"] # fields to include in training\n",
    "# TRAINING PARAM\n",
    "num_topics = 10 # truthfully we want to see 13 topics\n",
    "chunksize = 2000 # how many docs are processed at a time set to 2000 as default\n",
    "passes = 20 # how often the model is trained on all the docs set to 20 as default\n",
    "iterations = 400 # how often do we iterate over each doc set to 400 as default\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "################################################################ TO CHANGE ################################################################\n",
    "\n",
    "\n",
    "import json #\n",
    "import nltk # for preprocessing\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer # for tokenization\n",
    "from nltk.stem.wordnet import WordNetLemmatizer # for lemmatizing\n",
    "from gensim.corpora import Dictionary # to construct dictionary\n",
    "from gensim.models import LdaModel # to make LDA model\n",
    "from pprint import pprint # print output in a readable way\n",
    "from nltk.util import ngrams\n",
    "\n",
    "with open(path_json) as fp:\n",
    "    papers = [json.loads(line) for line in fp.readlines()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Methods - Abstract\n",
    "1. n-grams\n",
    "1. remove articles + small words\n",
    "\n",
    "## Preprocessing Methods - Authors\n",
    "1. n-grams\n",
    "1. Throwing out authors all together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ident(z,*args):\n",
    "    '''dummy identity function'''\n",
    "    if (type(z) is not list):\n",
    "        return z\n",
    "    else:\n",
    "        return ' '.join(z)\n",
    "    \n",
    "def author_iden(z,*args):\n",
    "    return z \n",
    "\n",
    "def add_ngrams(inpt_sentence, n=1):\n",
    "    \n",
    "    if inpt_sentence is not None:\n",
    "        \n",
    "        out=inpt_sentence\n",
    "        \n",
    "        for i in range(n,1,-1):\n",
    "            \n",
    "            grams=ngrams(inpt_sentence, i)\n",
    "            \n",
    "            out.extend(['_'.join(x) for x in grams])\n",
    "        \n",
    "        return ' '.join(out)\n",
    "    return ''\n",
    "\n",
    "def author_ngram(input_list, *args):\n",
    "    return [x.replace(' ', '_').lower() for x in input_list]\n",
    "\n",
    "def destroy_param(z,*args):\n",
    "    return []\n",
    "\n",
    "def preprocess_data(all_docs, min_word_len=2,\n",
    "                    title_pp=ident,arg_title=None,\n",
    "                    abstract_pp=ident, arg_abstract=None,\n",
    "                    author_pp=author_iden, arg_author=None):\n",
    "    ret_ar=[]\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for doc in all_docs:\n",
    "        \n",
    "        #title\n",
    "        title= ' '.join([lemmatizer.lemmatize (x) for x in doc['title'].split(' ')])\n",
    "        \n",
    "        #abstract\n",
    "        abstract= [lemmatizer.lemmatize (x) for x in str(doc['abstract']).split(' ')] #list\n",
    "        \n",
    "        # concatenate all strings \n",
    "        representation = title_pp(title,arg_title) + ' \\n '+ abstract_pp(abstract,arg_abstract).lower()\n",
    "        \n",
    "        # get rid of punctuation & tokenize\n",
    "        representation=tokenizer.tokenize(representation.lower()) + author_pp(doc['authors'],arg_author)\n",
    "        \n",
    "        # take out numbers (but not numbers within words)\n",
    "        representation = [token for token in representation if not token.isnumeric()]\n",
    "\n",
    "        # take out words that are at least 3 characters long character\n",
    "        representation = [token for token in representation if len(token) > min_word_len] \n",
    "\n",
    "        # channge code here to not lemmatize ngrams\n",
    "        #representation = [lemmatizer.lemmatize(token) for token in representation]\n",
    "\n",
    "        representation=[x.strip('_') for x in representation]\n",
    "        ret_ar.append(representation)\n",
    "    \n",
    "    return ret_ar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1 : \n",
    "Author ngram  + standard abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 2395\n",
      "Number of documents: 3154\n",
      "Average topic coherence: -2.0674.\n",
      "[(0,\n",
      "  '0.029*\"text\" + 0.022*\"analysis\" + 0.014*\"topic\" + 0.012*\"method\" + '\n",
      "  '0.011*\"content\" + 0.008*\"model\" + 0.007*\"political\" + 0.007*\"word\" + '\n",
      "  '0.007*\"approach\" + 0.007*\"news\"'),\n",
      " (1,\n",
      "  '0.029*\"energy\" + 0.016*\"power\" + 0.015*\"system\" + 0.010*\"cost\" + '\n",
      "  '0.009*\"environmental\" + 0.008*\"process\" + 0.008*\"design\" + 0.008*\"emission\" '\n",
      "  '+ 0.007*\"water\" + 0.006*\"optimization\"'),\n",
      " (2,\n",
      "  '0.036*\"disclosure\" + 0.020*\"firm\" + 0.019*\"environmental\" + 0.016*\"company\" '\n",
      "  '+ 0.016*\"corporate\" + 0.016*\"carbon\" + 0.015*\"information\" + '\n",
      "  '0.013*\"reporting\" + 0.012*\"financial\" + 0.012*\"study\"'),\n",
      " (3,\n",
      "  '0.054*\"data\" + 0.033*\"learning\" + 0.020*\"machine\" + 0.011*\"can\" + '\n",
      "  '0.010*\"method\" + 0.009*\"using\" + 0.009*\"based\" + 0.008*\"our\" + '\n",
      "  '0.007*\"approach\" + 0.007*\"algorithm\"'),\n",
      " (4,\n",
      "  '0.024*\"political\" + 0.018*\"party\" + 0.014*\"policy\" + 0.011*\"their\" + '\n",
      "  '0.009*\"more\" + 0.008*\"public\" + 0.008*\"news\" + 0.007*\"how\" + '\n",
      "  '0.007*\"coverage\" + 0.007*\"issue\"'),\n",
      " (5,\n",
      "  '0.027*\"model\" + 0.015*\"time\" + 0.015*\"prediction\" + 0.011*\"network\" + '\n",
      "  '0.010*\"method\" + 0.009*\"based\" + 0.008*\"using\" + 0.008*\"used\" + '\n",
      "  '0.007*\"seasonal\" + 0.007*\"neural\"'),\n",
      " (6,\n",
      "  '0.021*\"model\" + 0.019*\"forecast\" + 0.019*\"temperature\" + 0.019*\"climate\" + '\n",
      "  '0.018*\"skill\" + 0.018*\"over\" + 0.015*\"surface\" + 0.014*\"predictability\" + '\n",
      "  '0.013*\"precipitation\" + 0.012*\"variability\"'),\n",
      " (7,\n",
      "  '0.051*\"climate\" + 0.045*\"change\" + 0.014*\"carbon\" + 0.012*\"education\" + '\n",
      "  '0.009*\"global\" + 0.009*\"their\" + 0.007*\"environmental\" + 0.007*\"study\" + '\n",
      "  '0.007*\"have\" + 0.007*\"policy\"'),\n",
      " (8,\n",
      "  '0.022*\"social\" + 0.019*\"research\" + 0.013*\"analysis\" + 0.013*\"topic\" + '\n",
      "  '0.013*\"study\" + 0.008*\"data\" + 0.007*\"medium\" + 0.006*\"these\" + 0.006*\"can\" '\n",
      "  '+ 0.006*\"have\"'),\n",
      " (9,\n",
      "  '0.023*\"summer\" + 0.020*\"event\" + 0.013*\"data\" + 0.011*\"season\" + '\n",
      "  '0.010*\"interannual\" + 0.010*\"effect\" + 0.010*\"causal\" + 0.009*\"using\" + '\n",
      "  '0.008*\"winter\" + 0.008*\"bill\"')]\n"
     ]
    }
   ],
   "source": [
    "#corpus preprocessing\n",
    "full_data=preprocess_data(papers,\n",
    "                          author_pp= author_ngram )\n",
    "\n",
    "#constructs word to ID mapping \n",
    "dictionary = Dictionary(full_data)\n",
    "\n",
    "# filters out words that occur less than 20 times or are in more than 50% of docs\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "# transform to vectorized form to put in model\n",
    "corpus = [dictionary.doc2bow(doc) for doc in full_data]\n",
    "\n",
    "# Finds how many unique tokens we've found and how many docs we have\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))\n",
    "\n",
    "# index to word dictionary\n",
    "temp = dictionary[0] \n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")\n",
    "\n",
    "# sum of topic coherences of all topics, divided by the number of topics\n",
    "avg_topic_coherence = sum([t[1] for t in model.top_topics(corpus)]) / num_topics \n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "pprint(model.print_topics())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2 : \n",
    "Author ngram  + Abstract ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 5201\n",
      "Number of documents: 3154\n",
      "Average topic coherence: -1.8978.\n",
      "[(0,\n",
      "  '0.019*\"model\" + 0.015*\"sst\" + 0.014*\"temperature\" + 0.011*\"enso\" + '\n",
      "  '0.011*\"skill\" + 0.011*\"climate\" + 0.009*\"precipitation\" + 0.009*\"ocean\" + '\n",
      "  '0.009*\"however\" + 0.009*\"seasonal\"'),\n",
      " (1,\n",
      "  '0.021*\"disclosure\" + 0.015*\"carbon\" + 0.013*\"firm\" + 0.012*\"ghg\" + '\n",
      "  '0.009*\"company\" + 0.009*\"firms\" + 0.008*\"csr\" + 0.008*\"companies\" + '\n",
      "  '0.008*\"cdp\" + 0.008*\"information\"'),\n",
      " (2,\n",
      "  '0.022*\"however\" + 0.011*\"term\" + 0.011*\"social\" + 0.010*\"study\" + '\n",
      "  '0.009*\"non\" + 0.009*\"this_study\" + 0.008*\"based\" + 0.008*\"crisis\" + '\n",
      "  '0.007*\"health\" + 0.007*\"long\"'),\n",
      " (3,\n",
      "  '0.014*\"we\" + 0.011*\"party\" + 0.010*\"political\" + 0.010*\"in\" + '\n",
      "  '0.008*\"however\" + 0.007*\"policy\" + 0.007*\"government\" + 0.006*\"in_the\" + '\n",
      "  '0.005*\"public\" + 0.005*\"which\"'),\n",
      " (4,\n",
      "  '0.014*\"model\" + 0.013*\"data\" + 0.012*\"we\" + 0.011*\"learning\" + 0.010*\"in\" + '\n",
      "  '0.009*\"based\" + 0.009*\"network\" + 0.008*\"which\" + 0.008*\"method\" + '\n",
      "  '0.008*\"machine\"'),\n",
      " (5,\n",
      "  '0.022*\"energy\" + 0.016*\"system\" + 0.012*\"systems\" + 0.011*\"power\" + '\n",
      "  '0.009*\"cost\" + 0.007*\"environmental\" + 0.007*\"based\" + 0.007*\"in\" + '\n",
      "  '0.007*\"water\" + 0.006*\"multi\"'),\n",
      " (6,\n",
      "  '0.027*\"change\" + 0.019*\"climate\" + 0.017*\"climate_change\" + '\n",
      "  '0.011*\"research\" + 0.009*\"study\" + 0.009*\"education\" + 0.005*\"however\" + '\n",
      "  '0.005*\"it\" + 0.005*\"students\" + 0.005*\"environmental\"'),\n",
      " (7,\n",
      "  '0.032*\"time\" + 0.014*\"lda\" + 0.012*\"in\" + 0.012*\"model\" + '\n",
      "  '0.010*\"furthermore\" + 0.009*\"real\" + 0.009*\"in_addition\" + '\n",
      "  '0.009*\"additionally\" + 0.009*\"predictability\" + 0.008*\"high\"'),\n",
      " (8,\n",
      "  '0.050*\"we\" + 0.021*\"first\" + 0.017*\"second\" + 0.013*\"finally\" + 0.011*\"our\" '\n",
      "  '+ 0.008*\"here\" + 0.008*\"however\" + 0.008*\"but\" + 0.007*\"policy\" + '\n",
      "  '0.006*\"these\"'),\n",
      " (9,\n",
      "  '0.021*\"we\" + 0.019*\"data\" + 0.017*\"text\" + 0.015*\"analysis\" + 0.011*\"topic\" '\n",
      "  '+ 0.009*\"research\" + 0.007*\"however\" + 0.006*\"method\" + 0.006*\"in\" + '\n",
      "  '0.006*\"science\"')]\n"
     ]
    }
   ],
   "source": [
    "#corpus preprocessing\n",
    "full_data=preprocess_data(papers,\n",
    "                          author_pp= author_ngram,\n",
    "                          abstract_pp=add_ngrams,arg_abstract=3)\n",
    "\n",
    "#constructs word to ID mapping \n",
    "dictionary = Dictionary(full_data)\n",
    "\n",
    "# filters out words that occur less than 20 times or are in more than 50% of docs\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "# transform to vectorized form to put in model\n",
    "corpus = [dictionary.doc2bow(doc) for doc in full_data]\n",
    "\n",
    "# Finds how many unique tokens we've found and how many docs we have\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))\n",
    "\n",
    "# index to word dictionary\n",
    "temp = dictionary[0] \n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")\n",
    "\n",
    "# sum of topic coherences of all topics, divided by the number of topics\n",
    "avg_topic_coherence = sum([t[1] for t in model.top_topics(corpus)]) / num_topics \n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "pprint(model.print_topics())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3 : \n",
    "No author  + Abstract ngram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 5200\n",
      "Number of documents: 3154\n",
      "Average topic coherence: -1.9651.\n",
      "[(0,\n",
      "  '0.020*\"first\" + 0.016*\"second\" + 0.014*\"climate\" + 0.012*\"we\" + '\n",
      "  '0.009*\"however\" + 0.009*\"term\" + 0.008*\"here\" + 0.007*\"scale\" + '\n",
      "  '0.006*\"long\" + 0.006*\"model\"'),\n",
      " (1,\n",
      "  '0.021*\"data\" + 0.016*\"learning\" + 0.015*\"network\" + 0.012*\"time\" + '\n",
      "  '0.011*\"in\" + 0.011*\"machine\" + 0.008*\"method\" + 0.008*\"based\" + '\n",
      "  '0.007*\"which\" + 0.007*\"real\"'),\n",
      " (2,\n",
      "  '0.024*\"change\" + 0.019*\"climate_change\" + 0.015*\"disclosure\" + '\n",
      "  '0.015*\"climate\" + 0.012*\"carbon\" + 0.009*\"ghg\" + 0.009*\"firm\" + '\n",
      "  '0.007*\"information\" + 0.006*\"findings\" + 0.006*\"education\"'),\n",
      " (3,\n",
      "  '0.042*\"party\" + 0.017*\"parties\" + 0.013*\"right\" + 0.009*\"government\" + '\n",
      "  '0.009*\"position\" + 0.009*\"political\" + 0.008*\"left\" + 0.008*\"iii\" + '\n",
      "  '0.007*\"speech\" + 0.007*\"positions\"'),\n",
      " (4,\n",
      "  '0.025*\"we\" + 0.017*\"data\" + 0.014*\"text\" + 0.012*\"analysis\" + 0.009*\"topic\" '\n",
      "  '+ 0.008*\"however\" + 0.008*\"research\" + 0.007*\"our\" + 0.007*\"in\" + '\n",
      "  '0.006*\"method\"'),\n",
      " (5,\n",
      "  '0.039*\"we\" + 0.014*\"however\" + 0.010*\"political\" + 0.010*\"in\" + '\n",
      "  '0.008*\"news\" + 0.008*\"we_find\" + 0.007*\"our\" + 0.006*\"but\" + 0.006*\"public\" '\n",
      "  '+ 0.006*\"coverage\"'),\n",
      " (6,\n",
      "  '0.021*\"model\" + 0.015*\"temperature\" + 0.014*\"sst\" + 0.010*\"enso\" + '\n",
      "  '0.010*\"skill\" + 0.009*\"forecast\" + 0.009*\"precipitation\" + '\n",
      "  '0.009*\"prediction\" + 0.009*\"ocean\" + 0.008*\"seasonal\"'),\n",
      " (7,\n",
      "  '0.019*\"environmental\" + 0.011*\"economic\" + 0.011*\"csr\" + 0.009*\"social\" + '\n",
      "  '0.008*\"non\" + 0.008*\"study\" + 0.008*\"environment\" + 0.007*\"reporting\" + '\n",
      "  '0.007*\"research\" + 0.007*\"quality\"'),\n",
      " (8,\n",
      "  '0.008*\"in_the\" + 0.008*\"study\" + 0.008*\"policy\" + 0.007*\"it\" + 0.007*\"in\" + '\n",
      "  '0.006*\"analysis\" + 0.006*\"which\" + 0.006*\"how\" + 0.006*\"their\" + '\n",
      "  '0.005*\"to\"'),\n",
      " (9,\n",
      "  '0.014*\"energy\" + 0.012*\"system\" + 0.010*\"based\" + 0.009*\"however\" + '\n",
      "  '0.008*\"model\" + 0.008*\"in\" + 0.007*\"multi\" + 0.007*\"power\" + '\n",
      "  '0.007*\"systems\" + 0.006*\"cost\"')]\n"
     ]
    }
   ],
   "source": [
    "#corpus preprocessing\n",
    "full_data=preprocess_data(papers,\n",
    "                          author_pp= destroy_param,\n",
    "                          abstract_pp=add_ngrams,arg_abstract=3)\n",
    "\n",
    "#constructs word to ID mapping \n",
    "dictionary = Dictionary(full_data)\n",
    "\n",
    "# filters out words that occur less than 20 times or are in more than 50% of docs\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "# transform to vectorized form to put in model\n",
    "corpus = [dictionary.doc2bow(doc) for doc in full_data]\n",
    "\n",
    "# Finds how many unique tokens we've found and how many docs we have\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))\n",
    "\n",
    "# index to word dictionary\n",
    "temp = dictionary[0] \n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")\n",
    "\n",
    "# sum of topic coherences of all topics, divided by the number of topics\n",
    "avg_topic_coherence = sum([t[1] for t in model.top_topics(corpus)]) / num_topics \n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "pprint(model.print_topics())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of unique tokens not really changing may mean that the authors are not super important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4 : \n",
    "No author  + standard Abstract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 2395\n",
      "Number of documents: 3154\n",
      "Average topic coherence: -1.9941.\n",
      "[(0,\n",
      "  '0.020*\"party\" + 0.017*\"policy\" + 0.014*\"their\" + 0.012*\"political\" + '\n",
      "  '0.012*\"more\" + 0.011*\"public\" + 0.011*\"government\" + 0.008*\"how\" + '\n",
      "  '0.008*\"state\" + 0.007*\"group\"'),\n",
      " (1,\n",
      "  '0.034*\"energy\" + 0.014*\"power\" + 0.012*\"emission\" + 0.011*\"environmental\" + '\n",
      "  '0.011*\"cost\" + 0.010*\"system\" + 0.008*\"carbon\" + 0.008*\"gas\" + '\n",
      "  '0.008*\"water\" + 0.008*\"cycle\"'),\n",
      " (2,\n",
      "  '0.031*\"model\" + 0.021*\"learning\" + 0.019*\"data\" + 0.013*\"method\" + '\n",
      "  '0.013*\"machine\" + 0.010*\"using\" + 0.010*\"can\" + 0.010*\"our\" + '\n",
      "  '0.009*\"algorithm\" + 0.009*\"approach\"'),\n",
      " (3,\n",
      "  '0.032*\"disclosure\" + 0.026*\"carbon\" + 0.018*\"firm\" + 0.015*\"environmental\" '\n",
      "  '+ 0.014*\"corporate\" + 0.014*\"company\" + 0.013*\"information\" + 0.012*\"study\" '\n",
      "  '+ 0.011*\"reporting\" + 0.009*\"financial\"'),\n",
      " (4,\n",
      "  '0.046*\"climate\" + 0.041*\"change\" + 0.013*\"education\" + 0.011*\"study\" + '\n",
      "  '0.011*\"their\" + 0.008*\"environmental\" + 0.008*\"research\" + 0.007*\"were\" + '\n",
      "  '0.007*\"have\" + 0.007*\"development\"'),\n",
      " (5,\n",
      "  '0.020*\"text\" + 0.018*\"analysis\" + 0.018*\"topic\" + 0.015*\"political\" + '\n",
      "  '0.009*\"medium\" + 0.009*\"social\" + 0.008*\"study\" + 0.007*\"research\" + '\n",
      "  '0.007*\"content\" + 0.007*\"method\"'),\n",
      " (6,\n",
      "  '0.018*\"network\" + 0.015*\"system\" + 0.015*\"time\" + 0.010*\"predictability\" + '\n",
      "  '0.010*\"based\" + 0.010*\"control\" + 0.010*\"neural\" + 0.009*\"method\" + '\n",
      "  '0.008*\"disruption\" + 0.008*\"optimization\"'),\n",
      " (7,\n",
      "  '0.061*\"news\" + 0.039*\"none\" + 0.032*\"coding\" + 0.029*\"content\" + '\n",
      "  '0.025*\"human\" + 0.023*\"analysis\" + 0.021*\"automated\" + 0.015*\"die\" + '\n",
      "  '0.014*\"event\" + 0.013*\"que\"'),\n",
      " (8,\n",
      "  '0.024*\"model\" + 0.016*\"climate\" + 0.016*\"forecast\" + 0.013*\"temperature\" + '\n",
      "  '0.013*\"skill\" + 0.013*\"over\" + 0.011*\"prediction\" + 0.011*\"surface\" + '\n",
      "  '0.010*\"seasonal\" + 0.009*\"precipitation\"'),\n",
      " (9,\n",
      "  '0.040*\"data\" + 0.019*\"research\" + 0.012*\"social\" + 0.009*\"science\" + '\n",
      "  '0.008*\"method\" + 0.008*\"can\" + 0.008*\"learning\" + 0.008*\"information\" + '\n",
      "  '0.008*\"analysis\" + 0.007*\"based\"')]\n"
     ]
    }
   ],
   "source": [
    "#corpus preprocessing\n",
    "full_data=preprocess_data(papers,\n",
    "                          author_pp= destroy_param )\n",
    "\n",
    "#constructs word to ID mapping \n",
    "dictionary = Dictionary(full_data)\n",
    "\n",
    "# filters out words that occur less than 20 times or are in more than 50% of docs\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "# transform to vectorized form to put in model\n",
    "corpus = [dictionary.doc2bow(doc) for doc in full_data]\n",
    "\n",
    "# Finds how many unique tokens we've found and how many docs we have\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))\n",
    "\n",
    "# index to word dictionary\n",
    "temp = dictionary[0] \n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")\n",
    "\n",
    "# sum of topic coherences of all topics, divided by the number of topics\n",
    "avg_topic_coherence = sum([t[1] for t in model.top_topics(corpus)]) / num_topics \n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "pprint(model.print_topics())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micro",
   "language": "python",
   "name": "micro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
