{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/baddie/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "################################################################ TO CHANGE ################################################################\n",
    "# Stolen from Sneha\n",
    "# FILE PATHS AND FIELDS\n",
    "path_json = \"../comp767_papers_sample.jsonl\"  #3154 papers\n",
    "fields = [\"title\", \"abstract\", \"authors\"] # fields to include in training\n",
    "# TRAINING PARAM\n",
    "num_topics = 10 # truthfully we want to see 13 topics\n",
    "chunksize = 2000 # how many docs are processed at a time set to 2000 as default\n",
    "passes = 20 # how often the model is trained on all the docs set to 20 as default\n",
    "iterations = 400 # how often do we iterate over each doc set to 400 as default\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "################################################################ TO CHANGE ################################################################\n",
    "\n",
    "\n",
    "import json #\n",
    "import nltk # for preprocessing\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer # for tokenization\n",
    "from nltk.stem.wordnet import WordNetLemmatizer # for lemmatizing\n",
    "from gensim.corpora import Dictionary # to construct dictionary\n",
    "from gensim.models import LdaModel # to make LDA model\n",
    "from pprint import pprint # print output in a readable way\n",
    "from nltk.util import ngrams\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "with open(path_json) as fp:\n",
    "    papers = [json.loads(line) for line in fp.readlines()]\n",
    "\n",
    "np.random.seed(767) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ident(z,*args):\n",
    "    '''dummy identity function'''\n",
    "    if (type(z) is not list):\n",
    "        return z\n",
    "    else:\n",
    "        return ' '.join(z)\n",
    "    \n",
    "def author_iden(z,*args):\n",
    "    return z \n",
    "\n",
    "def add_ngrams(inpt_sentence, n=[1]):\n",
    "    \n",
    "    if inpt_sentence is not None:\n",
    "        \n",
    "        out=inpt_sentence\n",
    "        \n",
    "        for i in n:\n",
    "            \n",
    "            grams=ngrams(inpt_sentence, i)\n",
    "            \n",
    "            out.extend(['_'.join(x) for x in grams])\n",
    "        \n",
    "        return ' '.join(out)\n",
    "    return ''\n",
    "\n",
    "def author_ngram(input_list, *args):\n",
    "    return [x.replace(' ', '_').lower() for x in input_list]\n",
    "\n",
    "def destroy_param(z,*args):\n",
    "    return []\n",
    "\n",
    "def preprocess_data(all_docs, min_word_len=2,\n",
    "                    title_pp=ident,arg_title=None,\n",
    "                    abstract_pp=ident, arg_abstract=None,\n",
    "                    author_pp=author_iden, arg_author=None):\n",
    "    ret_ar=[]\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for doc in all_docs:\n",
    "        \n",
    "        #title\n",
    "        title= ' '.join([lemmatizer.lemmatize (x) for x in doc['title'].split(' ')])\n",
    "        \n",
    "        #abstract\n",
    "        abstract= [lemmatizer.lemmatize (x) for x in str(doc['abstract']).split(' ')] #list\n",
    "        \n",
    "        abstract = [x for x in abstract if len(x)>min_word_len]\n",
    "        \n",
    "        # concatenate all strings \n",
    "        representation = title_pp(title,arg_title) + ' \\n '+ abstract_pp(abstract,arg_abstract).lower()\n",
    "        \n",
    "        # get rid of punctuation & tokenize\n",
    "        representation=tokenizer.tokenize(representation.lower()) + author_pp(doc['authors'],arg_author)\n",
    "        \n",
    "        # take out numbers (but not numbers within words)\n",
    "        representation = [token for token in representation if not token.isnumeric()]\n",
    "\n",
    "        # take out words that are at least 3 characters long character\n",
    "        representation = [token for token in representation if len(token) > min_word_len] \n",
    "\n",
    "        # channge code here to not lemmatize ngrams\n",
    "        #representation = [lemmatizer.lemmatize(token) for token in representation]\n",
    "\n",
    "        representation=[x.strip('_') for x in representation]\n",
    "        ret_ar.append(representation)\n",
    "    \n",
    "    return ret_ar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hLDA(documents, layers, pp_args, ret=None, depth=0 ):\n",
    "    \n",
    "    print(layers)\n",
    "    \n",
    "    # start the with the base model\n",
    "    if len(layers) >1:\n",
    "        \n",
    "        if(ret is None):\n",
    "            ret=dict([(i, []) for i in range(len(layers))])\n",
    "        \n",
    "        full_data=preprocess_data(documents,**pp_args)\n",
    "        #constructs word to ID mapping \n",
    "        dictionary = Dictionary(full_data)\n",
    "\n",
    "        # filters out words that occur less than 20 times or are in more than 50% of docs\n",
    "        dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "        # transform to vectorized form to put in model\n",
    "        corpus = [dictionary.doc2bow(doc) for doc in full_data]\n",
    "\n",
    "        # Finds how many unique tokens we've found and how many docs we have\n",
    "        print('Number of unique tokens: %d' % len(dictionary))\n",
    "        print('Number of documents: %d' % len(corpus))\n",
    "\n",
    "        # index to word dictionary\n",
    "        temp = dictionary[0] \n",
    "        id2word = dictionary.id2token\n",
    "        \n",
    "        model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        chunksize=chunksize,\n",
    "        alpha='auto',\n",
    "        eta='auto',\n",
    "        iterations=iterations,\n",
    "        num_topics=layers[0],\n",
    "        passes=passes,\n",
    "        eval_every=eval_every\n",
    "        )\n",
    "        \n",
    "        ret[depth].append((model, corpus,dictionary))\n",
    "        \n",
    "        #list of list of documents in each topic \n",
    "        topic_sep = [[] for i in range(layers[0])]\n",
    "        for i in range(len(corpus)):\n",
    "            #find out which topic document corresponds to\n",
    "            doc_topic=np.argmax(model.inference(corpus[i:i+1])[0])\n",
    "            \n",
    "            #append document to \n",
    "            topic_sep[doc_topic].append(documents[i])\n",
    "        \n",
    "        # for each subtopic, we redo hLDA with new parameter\n",
    "        sub_models=[]\n",
    "        for top_docs in topic_sep:\n",
    "            ret=hLDA(top_docs, layers[1:] , pp_args, ret=ret, depth=depth+1)\n",
    "            \n",
    "        return ret\n",
    "    # leaves of tree\n",
    "    \n",
    "    else:\n",
    "        full_data=preprocess_data(documents,**pp_args)\n",
    "        #constructs word to ID mapping \n",
    "        dictionary = Dictionary(full_data)\n",
    "\n",
    "        # filters out words that occur less than 20 times or are in more than 50% of docs\n",
    "        dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "        # transform to vectorized form to put in model\n",
    "        corpus = [dictionary.doc2bow(doc) for doc in full_data]\n",
    "\n",
    "        # Finds how many unique tokens we've found and how many docs we have\n",
    "        print('Number of unique tokens: %d' % len(dictionary))\n",
    "        print('Number of documents: %d' % len(corpus))\n",
    "\n",
    "        # index to word dictionary\n",
    "        temp = dictionary[0] \n",
    "        id2word = dictionary.id2token\n",
    "        \n",
    "        model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        chunksize=chunksize,\n",
    "        alpha='auto',\n",
    "        eta='auto',\n",
    "        iterations=iterations,\n",
    "        num_topics=layers[0],\n",
    "        passes=passes,\n",
    "        eval_every=eval_every\n",
    "        )\n",
    "        \n",
    "        ret[depth].append((model, corpus,dictionary))\n",
    "        \n",
    "        return ret\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1\n",
      "[2, 13]\n",
      "Number of unique tokens: 2700\n",
      "Number of documents: 3154\n",
      "[13]\n",
      "Number of unique tokens: 1541\n",
      "Number of documents: 1533\n",
      "[13]\n",
      "Number of unique tokens: 1494\n",
      "Number of documents: 1621\n",
      "Test 2\n",
      "[13, 2]\n",
      "Number of unique tokens: 2700\n",
      "Number of documents: 3154\n",
      "[2]\n",
      "Number of unique tokens: 50\n",
      "Number of documents: 96\n",
      "[2]\n",
      "Number of unique tokens: 130\n",
      "Number of documents: 155\n",
      "[2]\n",
      "Number of unique tokens: 76\n",
      "Number of documents: 127\n",
      "[2]\n",
      "Number of unique tokens: 358\n",
      "Number of documents: 343\n",
      "[2]\n",
      "Number of unique tokens: 361\n",
      "Number of documents: 381\n",
      "[2]\n",
      "Number of unique tokens: 294\n",
      "Number of documents: 330\n",
      "[2]\n",
      "Number of unique tokens: 65\n",
      "Number of documents: 128\n",
      "[2]\n",
      "Number of unique tokens: 57\n",
      "Number of documents: 151\n",
      "[2]\n",
      "Number of unique tokens: 534\n",
      "Number of documents: 547\n",
      "[2]\n",
      "Number of unique tokens: 3\n",
      "Number of documents: 49\n",
      "[2]\n",
      "Number of unique tokens: 55\n",
      "Number of documents: 152\n",
      "[2]\n",
      "Number of unique tokens: 257\n",
      "Number of documents: 303\n",
      "[2]\n",
      "Number of unique tokens: 344\n",
      "Number of documents: 392\n",
      "Test 3\n",
      "[5, 5]\n",
      "Number of unique tokens: 2700\n",
      "Number of documents: 3154\n",
      "[5]\n",
      "Number of unique tokens: 489\n",
      "Number of documents: 512\n",
      "[5]\n",
      "Number of unique tokens: 323\n",
      "Number of documents: 334\n",
      "[5]\n",
      "Number of unique tokens: 563\n",
      "Number of documents: 583\n",
      "[5]\n",
      "Number of unique tokens: 784\n",
      "Number of documents: 709\n",
      "[5]\n",
      "Number of unique tokens: 991\n",
      "Number of documents: 1016\n",
      "Test 4\n",
      "[2, 10]\n",
      "Number of unique tokens: 2700\n",
      "Number of documents: 3154\n",
      "[10]\n",
      "Number of unique tokens: 1488\n",
      "Number of documents: 1567\n",
      "[10]\n",
      "Number of unique tokens: 1590\n",
      "Number of documents: 1587\n",
      "Test 5\n",
      "[10, 2]\n",
      "Number of unique tokens: 2700\n",
      "Number of documents: 3154\n",
      "[2]\n",
      "Number of unique tokens: 233\n",
      "Number of documents: 266\n",
      "[2]\n",
      "Number of unique tokens: 123\n",
      "Number of documents: 144\n",
      "[2]\n",
      "Number of unique tokens: 225\n",
      "Number of documents: 279\n",
      "[2]\n",
      "Number of unique tokens: 391\n",
      "Number of documents: 384\n",
      "[2]\n",
      "Number of unique tokens: 41\n",
      "Number of documents: 105\n",
      "[2]\n",
      "Number of unique tokens: 168\n",
      "Number of documents: 206\n",
      "[2]\n",
      "Number of unique tokens: 311\n",
      "Number of documents: 317\n",
      "[2]\n",
      "Number of unique tokens: 229\n",
      "Number of documents: 331\n",
      "[2]\n",
      "Number of unique tokens: 255\n",
      "Number of documents: 306\n",
      "[2]\n",
      "Number of unique tokens: 848\n",
      "Number of documents: 816\n"
     ]
    }
   ],
   "source": [
    "preprocess_args={'author_pp':author_ngram,\n",
    "                 'abstract_pp':add_ngrams,\n",
    "                 'arg_abstract':[3]}\n",
    "\n",
    "print('Test 1')\n",
    "test_1=hLDA(papers, [2,13], pp_args=preprocess_args, ret=None, depth=0 )\n",
    "\n",
    "print('Test 2')\n",
    "test_2=hLDA(papers, [13,2], pp_args=preprocess_args, ret=None, depth=0 )\n",
    "\n",
    "print('Test 3')\n",
    "test_3=hLDA(papers, [5,5], pp_args=preprocess_args, ret=None, depth=0 )\n",
    "\n",
    "print('Test 4')\n",
    "test_4=hLDA(papers, [2,10], pp_args=preprocess_args, ret=None, depth=0 )\n",
    "\n",
    "print('Test 5')\n",
    "test_5=hLDA(papers, [10,2], pp_args=preprocess_args, ret=None, depth=0 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_Hmodel(hMod, label):\n",
    "    \n",
    "    #hmod is dict tree output from hLDA\n",
    "    ret={'ave_level_coherences':[], 'ave_tree_coherence':0}\n",
    "    \n",
    "    level_coherence=[]\n",
    "    for key in hMod.keys():\n",
    "        \n",
    "        # all models and corresponding corpi at a specific depth\n",
    "        all_forks=hMod[key]\n",
    "        \n",
    "        all_coherences=[]\n",
    "        \n",
    "        #go through each fork and all corpus in each fork \n",
    "        sub_index=0\n",
    "        for mod, corps,dic in all_forks:\n",
    "            \n",
    "            #find average coherence within the fork\n",
    "            avg_topic_coherence = sum([t[1] for t in mod.top_topics(corps)]) / mod.num_topics \n",
    "            all_coherences.append(avg_topic_coherence)\n",
    "            \n",
    "            #viz current model\n",
    "            visualisation = pyLDAvis.gensim.prepare(mod, corps, dic)\n",
    "            \n",
    "            param_changes= 'depth_'+str(key)+'_topic_'+str(sub_index)+\"_atc_\"+str(round(avg_topic_coherence,4))\n",
    "            full_output_path =  \"./hierarchical_visualization/hLDA_Visualization_\" + label+ '_' + param_changes + \".html\"\n",
    "            pyLDAvis.save_html(visualisation, full_output_path)\n",
    "            \n",
    "            mod.save(\"./hierarchical_models/\"+label+\"/LDA_\" + param_changes + \".model\")\n",
    "            sub_index+=1\n",
    "            \n",
    "        ave_atc=sum(all_coherences)/len(all_coherences)\n",
    "        ret['ave_level_coherences'].append(ave_atc)\n",
    "        \n",
    "        level_coherence.append(ave_atc)\n",
    "        \n",
    "    tree_aatc=sum(level_coherence)/len(level_coherence)\n",
    "    ret['ave_tree_coherence']=tree_aatc\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ave_level_coherences': [-1.3321691344417008, -1.7692647396472396],\n",
       " 'ave_tree_coherence': -1.5507169370444702}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_Hmodel(test_1, 'test1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ave_level_coherences': [-2.5281125972957343, -1.1508300254505417],\n",
       " 'ave_tree_coherence': -1.839471311373138}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_Hmodel(test_2, 'test2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ave_level_coherences': [-1.5233495124080236, -1.4375305113037367],\n",
       " 'ave_tree_coherence': -1.48044001185588}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_Hmodel(test_3, 'test3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ave_level_coherences': [-1.4558969042236125, -1.8511646120094434],\n",
       " 'ave_tree_coherence': -1.653530758116528}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_Hmodel(test_4, 'test4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ave_level_coherences': [-1.8809799200481678, -1.1763031533491783],\n",
       " 'ave_tree_coherence': -1.528641536698673}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_Hmodel(test_5, 'test5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micro",
   "language": "python",
   "name": "micro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
